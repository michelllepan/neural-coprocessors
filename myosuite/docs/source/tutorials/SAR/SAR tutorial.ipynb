{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3084a00",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 42px;\">SAR tutorial</h1>\n",
    "\n",
    "In this notebook, we introduce synergistic action representations (SAR) and demonstrate how to use these representations to facilitate the training of high-dimensional continuous control policies within MyoSuite.\n",
    "\n",
    "The primary purpose of this notebook is to equip the user to train with SAR for their own task specification(s) both within and beyond the musculoskeletal control paradigm.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1 style=\"font-size: 28px;\">Contents of this notebook</h1>\n",
    "\n",
    "<ul style=\"line-height: 2; font-size: 16px;\">\n",
    "  <li><b>Imports and utilities</b>: set up tutorial with relevant imports and starter functions.\n",
    "  </li>\n",
    "  <li><b>Walkthrough of SAR core functions</b>: steps through and defines each of the four basic functions of the SAR method.\n",
    "  </li>\n",
    "  <li><b>Example 1: SAR x physiological locomotion</b>: end-to-end training example using the SAR method to learn forward locomotion on a diverse set of terrains.\n",
    "  </li>\n",
    "  <li><b>Example 2: SAR x physiological dexterity</b>: end-to-end training example using the SAR method to learn multiobject manipulation of parametric geometries.\n",
    "  </li>\n",
    "</ul>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5eddd",
   "metadata": {},
   "source": [
    "# Imports and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe8e7",
   "metadata": {},
   "source": [
    "First, we import our required and helper functions from `SAR_tutorial_utils.py`. If you encounter import errors, it is recommended to `pip install` any imports that are missing in your particular environment by uncommenting out the appropriate lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f610",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3==1.7.0\n",
    "# !pip install joblib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60114185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for SAR\n",
    "from SAR_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc299ac0",
   "metadata": {},
   "source": [
    "# SAR core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8433315",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "         Next, implement for SAR step by step. The framework follows the basic structure displayed below. Accordingly, we will implement functions that do each of the following:\n",
    "        <br>\n",
    "        <br>\n",
    "        <ol>\n",
    "            <li>Train a play phase policy.</li>\n",
    "            <li>Roll out play phase policy to capture muscle activations over time</li>\n",
    "            <li>Compute SAR from this activation dataset</li>\n",
    "            <li>Use computed SAR to train on target task</li>\n",
    "        </ol>\n",
    "        <br>\n",
    "        As we implement each of the above, we will also go into more detail about best practices for that step.\n",
    "        <br>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8cadd",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/framework.png\" alt=\"SAR framework\" style=\"width:1200px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125c47",
   "metadata": {},
   "source": [
    "### Step 1: Train a play phase policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24053a43",
   "metadata": {},
   "source": [
    "When selecting a play phase environment, it is advised to select a task that is both (a) simple enough that vanilla RL will be able to make reasonable progress on the task, and (b) sufficiently similar to the target task such that a synergistic representation learned from this simpler task will actually be informative for the target behavior. For example, in our manipulation pipeline, the play phase is a reorientation task composed of a small number of geometries, while the target reorientation task is composed of a much larger set of geometries. It is helpful to think of the play phase as a kind of curriculum learning that 'eases' the agent into learning useful representations for the target task.\n",
    "\n",
    "`train()` enables the simple training of a policy using the stable-baselines3 implementation of SAC on a desired environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, policy_name, timesteps, seed):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=1000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30fc82",
   "metadata": {},
   "source": [
    "### Step 2: Roll out play phase policy to capture muscle activations over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350c00b",
   "metadata": {},
   "source": [
    "`get_activations()` extracts muscle activation data at each timestep of the trained play phase policy's rollouts. In this implementation, `get_activations()` captures muscle activations from a given rollout if the rewards from that rollout fall above a certain threshold, computed as a percentile from a set of preview rollouts. \n",
    "\n",
    "Here, we set the number of sample episodes to 2000 and the reward percentile cutoff at 80%, though these should be considered hyperparameters that may require finetuning for a specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be60e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(name, env_name, seed, episodes=2000, percentile=80):\n",
    "    \"\"\"\n",
    "    Returns muscle activation data from N runs of a trained policy.\n",
    "\n",
    "    name: str; policy name (see train())\n",
    "    env_name: str; name of the gym environment\n",
    "    seed: str; seed of the trained policy\n",
    "    episodes: int; optional; how many rollouts?\n",
    "    percentile: int; optional; percentile to set the reward threshold for considering an episode as successful\n",
    "    \"\"\"\n",
    "    with gym.make(env_name) as env:\n",
    "        env.reset()\n",
    "\n",
    "        model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "        vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "\n",
    "        # Calculate the reward threshold from 100 preview episodes\n",
    "        preview_rewards = []\n",
    "        for _ in range(100):\n",
    "            env.reset()\n",
    "            rewards = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                rewards += r\n",
    "            preview_rewards.append(rewards)\n",
    "        reward_threshold = np.percentile(preview_rewards, percentile)\n",
    "\n",
    "        # Run the main episode loop\n",
    "        solved_acts = []\n",
    "        for _ in tqdm(range(episodes)):\n",
    "            env.reset()\n",
    "            rewards, acts = 0, []\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                acts.append(env.sim.data.act.copy())\n",
    "                rewards += r\n",
    "\n",
    "            if rewards > reward_threshold:\n",
    "                solved_acts.extend(acts)\n",
    "\n",
    "    return solved_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc01ec",
   "metadata": {},
   "source": [
    "### Step 3: Compute SAR from the activation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3cdc",
   "metadata": {},
   "source": [
    "We use linear representations (i.e., PCA, ICA, normalization) to approximate SAR. These representations are based directly on the motor neuroscience literature of representing muscle synergies (c.f., [Tresch et al, 2006](https://journals.physiology.org/doi/epdf/10.1152/jn.00222.2005)). Such representations also have the advantages of being highly interpretable and efficient for use as control signals. We also found in practice that nonlinear representations (such as VAE decoder networks) did not lead to strong performance are were far less efficient for training.\n",
    "\n",
    "Before computing SAR, we first seek to understand how informative each individual muscle synergy is for explaining the initial muscle activation dataset. `find_synergies` returns a dictionary (and optionally, a plot) that shows variance accounted for (VAF) by N synergies from the original muscle activation data. In general, we find that using a number of synergies where VAF > 80% leads to good performance, though this value should be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synergies(acts, plot=True):\n",
    "    \"\"\"\n",
    "    Computed % variance explained in the original muscle activation data with N synergies.\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    plot: bool; whether to plot the result\n",
    "    \"\"\"\n",
    "    syn_dict = {}\n",
    "    for i in range(acts.shape[1]):\n",
    "        pca = PCA(n_components=i+1)\n",
    "        _ = pca.fit_transform(acts)\n",
    "        syn_dict[i+1] =  round(sum(pca.explained_variance_ratio_), 4)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(list(syn_dict.keys()), list(syn_dict.values()))\n",
    "        plt.title('VAF by N synergies')\n",
    "        plt.xlabel('# synergies')\n",
    "        plt.ylabel('VAF')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    return syn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13361813",
   "metadata": {},
   "source": [
    "Once the number of synergies to use is determined (above), `compute_SAR()` will take the activation data as input and yield the synergistic action representation (SAR) in the form of ICA, PCA, and normalizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SAR(acts, n_syn, save=False):\n",
    "    \"\"\"\n",
    "    Takes activation data and desired n_comp as input and returns/optionally saves the ICA, PCA, and Scaler objects\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    n_comp: int; number of synergies to use\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_syn)\n",
    "    pca_act = pca.fit_transform(acts)\n",
    "    \n",
    "    ica = FastICA()\n",
    "    pcaica_act = ica.fit_transform(pca_act)\n",
    "    \n",
    "    normalizer = MinMaxScaler((-1, 1))    \n",
    "    normalizer.fit(pcaica_act)\n",
    "    \n",
    "    if save:\n",
    "        joblib.dump(ica, 'ica_demo.pkl') \n",
    "        joblib.dump(pca, 'pca_demo.pkl')\n",
    "        joblib.dump(normalizer, 'scaler_demo.pkl')\n",
    "    \n",
    "    return ica, pca, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60f8d5",
   "metadata": {},
   "source": [
    "### Step 4: Use computed SAR to train on target task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c987cc",
   "metadata": {},
   "source": [
    "In order to train with SAR, we initialize a `SynNoSynWrapper`, which implements the policy architecture presented below. \n",
    "\n",
    "The policy takes as input $o_t$ and outputs an $O+N$-dimensional action vector, $a_{t}^{O+N}$, where $O$ is the dimensionality of the original action manifold and $N$ is the dimensionality of the synergistic manifold. The first $N$ synergistic actions are passed through $SAR(a_{t}^{N})=a_{t}^{SAR}$ to recover the synergistic muscle activations. The subsequent $O$ task-specific activations are mixed with the task-general activations using a linear blend $\\varphi$ to recover the final action $a_t^*$ that steps the environment forward. Note that for the locomotion experiments presented next, it was actually found that purely relying on the synergistic, task-general pathway ($\\varphi=1$) was sufficient for learning robust locomotion (see end-to-end example later in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d499c17",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/pol_arch.png\" alt=\"Policy Architecture\" style=\"width:850px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynNoSynWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the combination of a task-general synergy space and a\n",
    "    task-specific orginal space, and uses this mix to step the environment in the original action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, scaler, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = scaler\n",
    "        self.weight = phi\n",
    "        \n",
    "        self.syn_act_space = self.pca.components_.shape[0]\n",
    "        self.no_syn_act_space = env.action_space.shape[0]\n",
    "        self.full_act_space = self.syn_act_space + self.no_syn_act_space\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.full_act_space,),dtype=np.float32)\n",
    "    def action(self, act):\n",
    "        syn_action = act[:self.syn_act_space]\n",
    "        no_syn_action = act[self.syn_act_space:]\n",
    "        \n",
    "        syn_action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([syn_action])))[0]\n",
    "        final_action = self.weight * syn_action + (1 - self.weight) * no_syn_action\n",
    "        \n",
    "        return final_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d80be",
   "metadata": {},
   "source": [
    "Additionally, we initialize a `SynergyWrapper`, which implements a variation of the above policy architecture that only leverages task-general synergistic activations (i.e., a more efficient implementation for handling the case where $\\varphi=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynergyWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the synergy space and inverse transforms\n",
    "    synergy-exploiting actions back into the original muscle activation space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = phi\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.pca.components_.shape[0],),dtype=np.float32)\n",
    "    \n",
    "    def action(self, act):\n",
    "        action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([act])))\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070abb8",
   "metadata": {},
   "source": [
    "We proceed to train a policy with this architecture using `SAR_RL`. We find that a blend weight $\\varphi$=0.66 between the synergistic (task-general) and nonsynergistic (task-specific) activations works best in practice, though this should also be considered a hyperparameter that may require optimization for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAR_RL(env_name, policy_name, timesteps, seed, ica, pca, normalizer, phi=.66, syn_nosyn=True):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC + SynNoSynWrapper.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    ica: the ICA object\n",
    "    pca: the PCA object\n",
    "    normalizer: the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    \"\"\"\n",
    "    if syn_nosyn:\n",
    "        env = SynNoSynWrapper(gym.make(env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = SynergyWrapper(gym.make(env_name), ica, pca, normalizer)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=5000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d9302",
   "metadata": {},
   "source": [
    "# Full training example 1: SAR x physiological locomotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c4882",
   "metadata": {},
   "source": [
    "Now that we have defined the core functions for implementing SAR, we will now train a locomotion policy using MyoLegs. In this example:\n",
    "- a policy is trained on straight flat walking task for 1.5M steps (`myoLegWalk-v0`)\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- Variance accounted for (VAF) by N synergies is computed. Here, we use 20 synergies from the 80-dimensional leg muscle activations\n",
    "- SAR is used to train on:\n",
    "    - a hilly terrain walking task (`myoLegHillyTerrainWalk-v0`)\n",
    "    - an uneven terrain walking task (`myoLegRoughTerrainWalk-v0`)\n",
    "    - a stair climbing task (`myoLegStairTerrainWalk-v0`)\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc17b86",
   "metadata": {},
   "source": [
    "# Step 1.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `myoLegWalk-v0` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de557",
   "metadata": {},
   "source": [
    "## Option 1.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29fc68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train('myoLegWalk-v0', 'play_period', 1.5e6, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_play_period_video'\n",
    "get_vid(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "muscle_data = get_activations(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc6df4",
   "metadata": {},
   "source": [
    "## Option 1.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = load_locomotion_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e199df",
   "metadata": {},
   "source": [
    "# Step 1.2: Train on unseen terrain with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose terrain to learn, or manually choose one\n",
    "new_terrain = np.random.choice(['Hilly', 'Stairs', 'Rough'])\n",
    "# new_terrain = ...\n",
    "print(f'myoLeg{new_terrain}TerrainWalk-v0 selected for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101f52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAR_RL(env_name=f'myoLeg{new_terrain}TerrainWalk-v0', policy_name='SAR-RL', timesteps=2.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66, syn_nosyn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'walk_SAR-RL_video'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name=f'myoLeg{new_terrain}TerrainWalk-v0', seed='0', episodes=5, video_name=video_name, \n",
    "        determ=False, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=False)\n",
    "\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10cb1e",
   "metadata": {},
   "source": [
    "# Step 1.3 (optional): Train on unseen terrain with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(f'myoLeg{new_terrain}TerrainWalk-v0', 'RL-E2E', 4e6, '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16629ae9",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac0f3",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9424151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='locomotion', terrain=new_terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf790b",
   "metadata": {},
   "source": [
    "# Full training example 2: SAR x physiological manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cbb1e",
   "metadata": {},
   "source": [
    "Now, we turn to using synergies for manipulation rather than locomotion. In this example:\n",
    "- a policy is trained on an eight-object reorientation task, `Reorient8-v0`.\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- variance accounted for (VAF) by N synergies is computed. Here, we select N where VAF > 0.8.\n",
    "- SAR is computed at this VAF threshold.\n",
    "- SAR is used to train on a 100-object reorientation task, `Reorient100-v0`.\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1963d71",
   "metadata": {},
   "source": [
    "# Step 2.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `Reorient8` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c960e8",
   "metadata": {},
   "source": [
    "## Option 2.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a policy is trained on an eight-object reorientation task, Reorient8-v0\n",
    "train(env_name='myoHandReorient8-v0', policy_name='play_period', timesteps=1e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f34571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after training, we collect muscle activation data from policy rollouts\n",
    "muscle_data = get_activations(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a109e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAF by N synergies is computed. Here, N is selected where VAF > 0.8\n",
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'play_period_vid'\n",
    "get_vid(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR is computed at this VAF threshold\n",
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e6fd8",
   "metadata": {},
   "source": [
    "## Option 2.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592deab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO USE PRECOMPUTED SAR\n",
    "ica,pca,normalizer = load_manipulation_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be153658",
   "metadata": {},
   "source": [
    "# Step 2.2: Train on Reorient100 with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf7de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SAR is used to train on a 100-object reorientation task, Reorient100-v0\n",
    "SAR_RL(env_name='myoHandReorient100-v0', policy_name='SAR-RL', timesteps=1.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'SAR_vid'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name='myoHandReorient100-v0', seed='0', episodes=10, video_name=video_name, \n",
    "        determ=True, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=True)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc00d6",
   "metadata": {},
   "source": [
    "# Step 2.3 (optional): Train on Reorient100 with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d1",
   "metadata": {},
   "source": [
    "We can compare the performance of (A) pretraining for 1M steps on `Reorient8-v0`, getting SAR, and training with SAR-RL on `Reorient100-v0` for 2M steps with (B) training for 3M steps using end-to-end RL ('RL-E2E')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d36ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(env_name='myoHandReorient100-v0', policy_name='RL-E2E', timesteps=2.5e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'RL+E2E_vid'\n",
    "get_vid('RL-E2E', 'myoHandReorient100-v0', '0', determ=False, episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c777",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1717e5",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05215d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='manipulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bc976",
   "metadata": {},
   "source": [
    "# Step 2.5 (optional): zero-shot testing manipulation policies on new unseen objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bba92d",
   "metadata": {},
   "source": [
    "We can optionally test these manipulation policies on unseen objects to determine their generalizability. Here, we implement two test\n",
    "environments.\n",
    "\n",
    "1. ReorientID-v0—1000 unseen objects generated by sampling from the same dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with the same kind of shapes as those seen in the training set.\n",
    "\n",
    "2. ReorientOOD-v0—1000 unseen objects generated by sampling from different dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with different kinds of shapes as those seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba66889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_test(name, test_env_name, env_name='myoHandReorient100-v0', seed='0', determ=True, ica=None, \n",
    "                  pca=None, normalizer=None, phi=.66, episodes=500, is_sar=False, syn_nosyn=False):\n",
    "    \"\"\"\n",
    "    Check zero-shot performance of policies on the test environments.\n",
    "    \n",
    "    name: str; name of the policy to test\n",
    "    env_name: str; name of gym env the policy to test was trained on (Reorient100-v0).\n",
    "    seed: str; seed of the policy to test\n",
    "    test_env_name: str; name of the desired test env\n",
    "    ica: if testing SAR-RL, the ICA object\n",
    "    pca: if testing SAR-RL, the PCA object\n",
    "    normalizer: if testing SAR-RL, the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    episodes: int; number of episodes to run on the test environment\n",
    "    \"\"\"\n",
    "    if is_sar:\n",
    "        if syn_nosyn:\n",
    "            env = SynNoSynWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "        else:\n",
    "            env = SynergyWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = gym.make(test_env_name)\n",
    "    env.reset()\n",
    "\n",
    "    model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "    vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "    solved = []\n",
    "    for i,_ in enumerate(range(episodes)):\n",
    "        is_solved = []\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            o = env.get_obs()\n",
    "            o = vec.normalize_obs(o)\n",
    "            a, __ = model.predict(o, deterministic=determ)\n",
    "            next_o, r, done, info = env.step(a)\n",
    "            is_solved.append(info['solved'])\n",
    "        \n",
    "        if sum(is_solved) > 0:\n",
    "            solved.append(1)\n",
    "        else:\n",
    "            solved.append(0)\n",
    "\n",
    "    env.close()\n",
    "    return np.mean(solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08578ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing RL-E2E policy\n",
    "\n",
    "name = 'RL-E2E'\n",
    "\n",
    "e2e_id = zeroshot_test(name, 'myoHandReorientID-v0')\n",
    "\n",
    "e2e_ood = zeroshot_test(name, 'myoHandReorientOOD-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c97370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing SAR-RL policy\n",
    "\n",
    "name = 'SAR-RL'\n",
    "\n",
    "sar_id = zeroshot_test(name, 'myoHandReorientID-v0', ica=ica, pca=pca, \n",
    "                       normalizer=normalizer, is_sar=True, syn_nosyn=True)\n",
    "\n",
    "sar_ood = zeroshot_test(name, 'myoHandReorientOOD-v0', ica=ica, pca=pca, \n",
    "                        normalizer=normalizer, is_sar=True, syn_nosyn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cfe1b",
   "metadata": {},
   "source": [
    "We can visualize these zero-shot generalization results by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18035c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "    \n",
    "zeroshot = {\"SAR-RL\": [sar_id,sar_ood],\n",
    "            \"RL-E2E\": [e2e_id,e2e_ood]}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_zeroshot(ax, zeroshot)\n",
    "fig.set_size_inches(6, 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
